<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Computer Vision final project</title>
<style>
  body {
    font-family: Arial, sans-serif;
  }
  .header {
    background-color: #333;
    color: white;
    padding: 10px;
    text-align: center;

  }
  .container {
    display: flex;
    flex-wrap: wrap;
    padding: 20px;
    justify-content: space-around;
  }
  .section {
    font-size: 20px;
    border: 1px solid #ddd;
    margin: 10px;
    padding: 15px;
    flex-basis: 45%; /* Adjust the width for each box */
    box-shadow: 0 2px 5px rgba(0,0,0,0.2);
  }

  .section:hover {
    background-color: #f0f0f0; /* Light grey background on hover */
    border-color: #9e0303; /* Change border color on hover */
    box-shadow: 0 4px 8px rgba(0,0,0,0.3); /* Add or change shadow on hover */
}

.section h2:hover {
    color: #9e0303; /* Change title color on hover */
}

  .section-link {
    text-decoration: none; /* Removes underline from links */
    color: inherit; /* Ensures the text color is not changed by the link */
  }

  .section-link:hover .section {
    background-color: #f5f5f5; /* Example hover effect for the section */
  }

  .section img {
    max-width: 100%;
    height: auto;
  }
  .footer {
    background-color: #333;
    color: white;
    padding: 10px;
    text-align: center;
  }

  .centered-image {
    display: block;
    margin-left: auto;
    margin-right: auto;
}

  .Abstract {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

  .Introduction {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

    .Approaches {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

    .Implementation {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    .Results {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    .Reference {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    
    .justified {
            text-align: justify; /* Justifies the paragraph text */
            margin-left: auto;   /* These two lines center the paragraph block */
            margin-right: auto;
            max-width: 1200px;    /* You can adjust this value as per your design needs */
            font-size: 22px;     /* Increased font size */
            line-height:1.6;
        }

    html {
    scroll-behavior: smooth;
  }

</style>
</head>
<body>

<div  class="header">
  <h1>A Comparative Study of Vehicle Detection and Tracking: Deep Learning vs.TraditionalÂ Methods
</h1>
<h2> Team Members: Diksha Aggarwal and Surafel Anshebo</h2>
<h2> Fall 2023 ECE 4554/5554 Computer Vision: Course Project</h2>
<h2> Virginia Tech</h2>

</div>
<br></br>

<div class="container">

  <div class="section">
    <a href="#Abstract"><h2>1. Abstract</h2></a>
    <p>Motivation behind the problem, approaches and results obtained.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>

    <div class="section">
    <a href="#Introduction"><h2>2. Introduction</h2></a>
    <p>A brief description of the problem being dealt with, which includes - 
        the motivation behind the problem, background on the existing 
        approaches being used in this field of research, and some applications of this 
        project.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>

  <div class="section">
    <a href="#Approaches"> <h2>3. Approaches</h2></a>
    <p>A description of the different approaches proposed...</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Implementation"> <h2>4. Implementation</h2></a>
    <p>A description of the dataset used in the project, 
        the algorithms, along with any tuning parameters used in this project.</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Results"> <h2>5. Results and Conclusion</h2></a>
    <p>A description of the different approaches proposed...</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Reference"><h2>6. Reference</h2></a>
    <p>A brief description of the problem being dealt with, which includes - 
        the motivation behind the problem, background on the existing 
        approaches being used in this field of research, and some applications of this 
        project.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>


</div>

    <!-- Abstract -->
    <h3 id="Abstract" class="Abstract">Abstract</h3>
    <p class="justified">
        The Unmanned Aerial Vehicles (UAVs) are rapidly emerging with their applications ranging from surveillance to disaster response. 
        One such area is real-time traffic monitoring where UAVs with their vision based 
        methods can play a significant role in streamlining traffic flow, mitigating 
        congestion and quick emergency response in accidents
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580516300887"> 
        (Liang Wang, Fangliang Chen, Huiming Yin, December 2016)</a> However, such an 
        implementation is accompanied by challenges related to accuracy and computational 
        overload. Therefore, a comprehensive comparison of available methodologies in 
        computer vision is essential to determine the most effective approach for traffic 
        management tasks.
    </p>
    <br><br>

    <!-- Introduction -->
    <h3 id="Introduction" class ="Introduction">Introduction</h3>
    <p class="justified">Classical computer vision methods, which leverage inherent 
        image attributes such as texture, color, and shape, have been the mainstay for years. 
        Vehicles, with their distinct symmetrical shapes and unique colors, are 
        particularly amenable to these techniques. There are several traditional 
        computer vision methods that are being used in vehicle detection. 
        Researchers have used methods such as template matching and Haar cascade, 
        HOG based features, SIFT, ORB with classifiers like KNN, SVM to do 
        vehicle detection. While template matching provides good results, 
        Haar cascade is a better approach as it detects the objects by 
        detecting features. HOG and Haar cascades give competitive results. 
        Detected objects can be tracked with methods like Lucas-Kanade 
        optical flow, Kalman filter based SORT, mean-shift tracking. 
        These methods are mostly based on prediction of the next 
        probable position based on tracking features. On the other 
        hand, in deep learning based methods like RCNN, fast RCNN, 
        which is a two-stage method and one staged method like YOLO.
        This research conducts a comparative analysis of traditional 
        computer vision 
        <p class="justified">Both techniques exhibit limitations and 
            advantages 
            associated with hardware and software. While classical computer vision
             methods struggle with occlusion and lighting problems, they serve well in terms of computation power. Machine learning based methods give robust solutions and are not affected by the surrounding environment. But they are highly dataset dependent and need a lot of computational capability to train huge models. The foundation of the proposed research lies in evaluating the above two models, via both qualitative and quantitative analysis, to perform real-time vehicle detection and tracking.
            and deep learning based vehicle detection and tracking algorithms using UAV for real-time traffic monitoring application. In conventional computer vision methods, the Haar cascade method is used with the Kannade Lucas optical flow tracker. This technique is compared with deep learning-based methods YoloV7 with DeepSORT to detect and track vehicles. A qualitative and quantitative analysis is performed in simulation and then tested on an in-house built UAV. The goal is to support informed decision making in perception and planning, ultimately enhancing safety in the industry. By evaluating these methods on a UAV platform, the research provides valuable insights for industry stakeholders to choose the most suitable approach for their specific requirements.
        
        Both techniques exhibit limitations and advantages associated with hardware and software. While classical computer vision methods struggle with occlusion and lighting problems, they serve well in terms of computation power. Machine learning based methods give robust solutions and are not affected by the surrounding environment. But they are highly dataset dependent and need a lot of computational capability to train huge models. The foundation of the proposed research lies in evaluating the above two models, via both qualitative and quantitative analysis, to perform real-time vehicle detection and tracking.</p>
    <br><br>


    <!-- Approaches -->
    <h3 id="Approaches" class ="Approaches">Approaches</h3>
    <h3 class="justified">Deep learning approach</h3>

    <h3 class="justified">Overview of YOLOv7:</h3>
<p class="justified">YOLOv7 is an advanced version of the YOLO architecture. Unlike traditional two-stage detectors like R-CNN, which first select region proposals and then classify them, YOLOv7 is a single-stage detector that predicts both bounding boxes and class probabilities directly from the image in one evaluation. This makes it exceptionally fast and suitable for real-time applications.
</p>
      <h3 class="justified">Overview of DeepSort:</h3>
<p class="justified"> DeepSort is an extension of the SORT (Simple Online and Realtime Tracking) algorithm. While SORT uses Kalman filtering and Hungarian algorithm for tracking, DeepSort incorporates deep learning features to improve tracking performance, especially in cases of occlusions or interactions between objects.
</p>
<img  src="deepsort_blockdiag.png" alt="Block Diagram of Deep SORT (Reference of paper 
https://www.hindawi.com/journals/cin/2023/7974201/fig1/ )" class="centered-image" style="width: 50%; height: auto;">
    
     <h3 class="justified"> Tracking with Deep SORT:</h3>

<h4 class="justified"> Initializing Trackers: </h4> <p class="justified"> When an object is detected for the first time, Deep SORT initializes a new tracker for it. This tracker uses the extracted features to keep track of the object. </p>
<h4 class="justified"> Data Association:</h4> <p class="justified"> In each new frame, Deep SORT performs a data association step. It compares the new detections (from YOLO) with existing trackers. This comparison is based on both the appearance features and the predicted motion of the objects. The Kalman filter is used for predicting the motion.</p>
<h4 class="justified">Matching:</h4> <p class="justified"> Deep SORT matches the new detections with existing trackers. If a detection matches an existing tracker, it updates the tracker's state (like its new position and appearance). If there are detections that don't match any existing tracker, it creates new trackers for them.</p>
<h4 class="justified"> Handling Lost Tracks:</h4> <p class="justified"> Sometimes, an object might be occluded or move out of the frame. Deep SORT handles this by allowing trackers to exist for a short time without new matching detections, giving the object a chance to reappear. If a tracker doesn't get a matching detection for too long, it is removed.</p>


  <div class="justified">
  <div style="display: flex; justify-content: space-around; align-items: center;">
    <figure id="fig1">
        <img src="sample_input.png" alt="Sample Input" style="width: 30%; height: auto; margin-right: 10px;">
        <figcaption>Figure 1: Sample Input</figcaption>
    </figure>

    <figure id="fig2">
        <img src="sample_output.png" alt="Sample Output" style="width: 30%; height: auto;">
        <figcaption>Figure 2: Sample Output</figcaption>
    </figure>
  </div>

    <div style="display: flex; justify-content: space-around; align-items: center;">
    <figure id="fig1">
        <img src="img15.jpg" alt="Sample Input" style="width: 30%; height: auto; margin-right: 5px;">
    </figure>

    <figure id="fig2">
        <img src="img671.jpg" alt="Sample Output" style="width: 30%; height: auto;">
    </figure>

      <figure id="fig3">
        <img src="img83.jpg" alt="Sample Output" style="width: 30%; height: auto;">
    </figure>

      <figure id="fig4">
        <img src="img923.jpg" alt="Sample Output" style="width: 30%; height: auto;">
    </figure>
  </div>
  <figcaption style="text-align: center;">Common Caption: Overview of Dataset</figcaption>
  </div>

  
      <p class="justified">
A github repository: https://github.com/John1liu/YOLOV5-DeepSORT-Vehicle-Tracking-Master.git is used to perform the integration of DeepSORT with YoloV5. 
  The author of this repository has trained the YoloV5 and generated weights to perform detection and tracking. 
  But the model did not perform any predictions on videos other than the tested video in the code. 
  Thus, weights are generated by training the YoloV5. 
      </p>

<h4 class="justified"> Training of YoloV5: </h4>
<div class="justified">
<p> Since YoloV5 is not trained on vehicle dataset, training is performed in Colab. YoloV5 is trained with the vehicle dataset from a github repository:
https://github.com/MaryamBoneh/Vehicle-Detection.git. YoloV5 provides a tutorial on google colab. This colab file is used to train the images. 

Data: The vehicle dataset consists of labeled images. 
Data Augmentation:

The model is trained on the custom dataset. The pre-trained weights are fine tuned using this dataset. 

The dataset contains around 1300 images. The dataset is augmented in Roboflow with rotation, brightness etc. The dataset is divided into 70% training images, 20% validation images and 10% testing images. The training was done for 50 epochs in the batch of 16 and it took 1.25 hours to train the model. 
https://colab.research.google.com/drive/1mOU64O4PNLYIvsotNcXOS6FVumm8eLV0?usp=sharing
https://colab.research.google.com/drive/1kZnDnera2oelmQyJsM13xPc5ABvXyRdg?usp=sharing


YoloV5 training accuracy:

F1 Score: The F1 score is a harmonic mean of precision and recall. A higher F1 score indicates a better balance between precision and recall. It is especially useful when the class distribution is imbalanced.

</p>
  <\div>

<h2 class="justified">Computer Vision Approach</h2>

<h4 class="justified">1. Detection using Haar Cascade: </h4>
<h4 class="justified">2. Tracking using Lucas-Kanade method </h4>

<h4 class="justified">1.1 Collect image dataset: </h4>

<p class="justified"> The first step involves gathering a large dataset of images. Positive samples are images 
  containing the objects of interest (e.g. Cars), and negative samples are 
  those without the objects. It's crucial to have a diverse set of images to train the model effectively.
  </p>      
      
      <h4 class="justified">1.2 Feature selection: </h4>

      <p class="justified"> Haar-like features are used in this step. These features are simple 
        rectangular patterns that are superimposed on images to calculate the difference in pixel intensities. 
        The algorithm looks for specific features within an image, such as edges or changes in texture.
        </p>

        <h4 class="justified">1.3 Create an integral image: </h4>

        <p class="justified"> This step involves converting each image in the dataset into an integral image. 
          An integral image is a representation that allows the algorithm to calculate the sum of pixel values in 
          any given rectangle within the image very efficiently. This process speeds up the feature calculation significantly. 
          </p>

        <h4 class="justified">1.4 Training the classifier: </h4>

        <p class="justified"> With the features and integral images, a classifier (usually AdaBoost) is trained. 
          This classifier learns which features are most effective in distinguishing between positive and negative samples. 
          The training process involves selecting a small set of the most effective features from a larger pool, 
          as using all possible features would be computationally intensive and inefficient. 

          </p>

          <h4 class="justified">1.5 Cascading classifiers: </h4>

          <p class="justified"> Finally, the trained classifiers are arranged in a cascade. This means the image 
            passes through multiple stages of classifiers. Each stage eliminates non-object regions from 
            consideration, thus reducing false positives. The cascading process ensures that only those 
            regions of the image that strongly resemble the object of interest (as learned by the classifiers) 
            are passed to the final detection stage.
  
            </p>
            <br><br>

        <!-- Implementation -->
        <h3 id="Implementation" class ="Implementation">Implementation</h3>
        <p class="justified">Classical computer vision methods, which leverage inherent image attributes such as texture, color, and shape, have been the mainstay for years. Vehicles, with their distinct symmetrical shapes and unique colors, are particularly amenable to these techniques. There are several traditional computer vision methods that are being used in vehicle detection. Researchers have used methods such as template matching and Haar cascade, HOG based features, SIFT, ORB with classifiers like KNN, SVM to do vehicle detection. While template matching provides good results, Haar cascade is a better approach as it detects the objects by detecting features. HOG and Haar cascades give competitive results. Detected objects can be tracked with methods like Lucas-Kanade optical flow, Kalman filter based SORT, mean-shift tracking. These methods are mostly based on prediction of the next probable position based on tracking features. On the other hand, in deep learning based methods like RCNN, fast RCNN, which is a two-stage method and one staged method like YOLO.</p>
    
        <br><br>


            <!-- Results -->
    <h3 id="Results" class ="Results">Results and Conclusion</h3>
    <p class="justified">Classical computer vision methods, which leverage inherent image attributes such as texture, color, and shape, have been the mainstay for years. Vehicles, with their distinct symmetrical shapes and unique colors, are particularly amenable to these techniques. There are several traditional computer vision methods that are being used in vehicle detection. Researchers have used methods such as template matching and Haar cascade, HOG based features, SIFT, ORB with classifiers like KNN, SVM to do vehicle detection. While template matching provides good results, Haar cascade is a better approach as it detects the objects by detecting features. HOG and Haar cascades give competitive results. Detected objects can be tracked with methods like Lucas-Kanade optical flow, Kalman filter based SORT, mean-shift tracking. These methods are mostly based on prediction of the next probable position based on tracking features. On the other hand, in deep learning based methods like RCNN, fast RCNN, which is a two-stage method and one staged method like YOLO.</p>

    <br><br>


                <!-- Reference -->
    <h3 id="Reference" class ="Reference">Reference</h3>

    <br><br>
            
    
<div class="footer">
  <p>Â© Diksha Aggarwal and Surafel Anshebo</p>
</div>

</body>
</html>
