<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Computer Vision final project</title>
<style>
  body {
    font-family: Arial, sans-serif;
  }
  .header {
    background-color: #333;
    color: white;
    padding: 10px;
    text-align: center;

  }
  .container {
    display: flex;
    flex-wrap: wrap;
    padding: 20px;
    justify-content: space-around;
  }
  .section {
    font-size: 20px;
    border: 1px solid #ddd;
    margin: 10px;
    padding: 15px;
    flex-basis: 45%; /* Adjust the width for each box */
    box-shadow: 0 2px 5px rgba(0,0,0,0.2);
  }

  .section:hover {
    background-color: #f0f0f0; /* Light grey background on hover */
    border-color: #9e0303; /* Change border color on hover */
    box-shadow: 0 4px 8px rgba(0,0,0,0.3); /* Add or change shadow on hover */
}

.section h2:hover {
    color: #9e0303; /* Change title color on hover */
}

  .section-link {
    text-decoration: none; /* Removes underline from links */
    color: inherit; /* Ensures the text color is not changed by the link */
  }

  .section-link:hover .section {
    background-color: #f5f5f5; /* Example hover effect for the section */
  }

  .section img {
    max-width: 100%;
    height: auto;
  }
  .footer {
    background-color: #333;
    color: white;
    padding: 10px;
    text-align: center;
  }

  .Abstract {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

  .Introduction {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

    .Approaches {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }

    .Implementation {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    .Results {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    .Reference {
      font-size: 40px;
      text-align: center; /* Center align the text */
      color: #9e0303; /* Change the color (example: red) */
    }
    
    .justified {
            text-align: justify; /* Justifies the paragraph text */
            margin-left: auto;   /* These two lines center the paragraph block */
            margin-right: auto;
            max-width: 1200px;    /* You can adjust this value as per your design needs */
            font-size: 22px;     /* Increased font size */
            line-height:1.6;
        }

    html {
    scroll-behavior: smooth;
  }

</style>
</head>
<body>

<div  class="header">
  <h1>A Comparative Study of Vehicle Detection and Tracking: Deep Learning vs.Traditional Methods
</h1>
<h2> Team Members: Diksha Aggarwal and Surafel Anshebo</h2>
<h2> Fall 2023 ECE 4554/5554 Computer Vision: Course Project</h2>
<h2> Virginia Tech</h2>

</div>
<br></br>

<div class="container">

  <div class="section">
    <a href="#Abstract"><h2>1. Abstract</h2></a>
    <p>Motivation behind the problem, approaches and results obtained.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>

    <div class="section">
    <a href="#Introduction"><h2>2. Introduction</h2></a>
    <p>A brief description of the problem being dealt with, which includes - 
        the motivation behind the problem, background on the existing 
        approaches being used in this field of research, and some applications of this 
        project.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>

  <div class="section">
    <a href="#Approaches"> <h2>3. Approaches</h2></a>
    <p>A description of the different approaches proposed...</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Implementation"> <h2>4. Implementation</h2></a>
    <p>A description of the dataset used in the project, 
        the algorithms, along with any tuning parameters used in this project.</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Results"> <h2>5. Results and Conclusion</h2></a>
    <p>A description of the different approaches proposed...</p>
    <img src="path-to-approaches-image.jpg" alt="Approaches Image">
  </div>

  <div class="section">
    <a href="#Reference"><h2>6. Reference</h2></a>
    <p>A brief description of the problem being dealt with, which includes - 
        the motivation behind the problem, background on the existing 
        approaches being used in this field of research, and some applications of this 
        project.</p>
    <img src="path-to-intro-image.jpg" alt="Intro Image">
  </div>


</div>

    <!-- Abstract -->
    <h3 id="Abstract" class="Abstract">Abstract</h3>
    <p class="justified">
        The Unmanned Aerial Vehicles (UAVs) are rapidly emerging with their applications ranging from surveillance to disaster response. 
        One such area is real-time traffic monitoring where UAVs with their vision based 
        methods can play a significant role in streamlining traffic flow, mitigating 
        congestion and quick emergency response in accidents
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S0926580516300887"> 
        (Liang Wang, Fangliang Chen, Huiming Yin, December 2016)</a> However, such an 
        implementation is accompanied by challenges related to accuracy and computational 
        overload. Therefore, a comprehensive comparison of available methodologies in 
        computer vision is essential to determine the most effective approach for traffic 
        management tasks.
    </p>
    <br><br>

    <!-- Introduction -->
    <h3 id="Introduction" class ="Introduction">Introduction</h3>
    <p class="justified">Classical computer vision methods, which leverage inherent 
        image attributes such as texture, color, and shape, have been the mainstay for years. 
        Vehicles, with their distinct symmetrical shapes and unique colors, are 
        particularly amenable to these techniques. There are several traditional 
        computer vision methods that are being used in vehicle detection. 
        Researchers have used methods such as template matching and Haar cascade, 
        HOG based features, SIFT, ORB with classifiers like KNN, SVM to do 
        vehicle detection. While template matching provides good results, 
        Haar cascade is a better approach as it detects the objects by 
        detecting features. HOG and Haar cascades give competitive results. 
        Detected objects can be tracked with methods like Lucas-Kanade 
        optical flow, Kalman filter based SORT, mean-shift tracking. 
        These methods are mostly based on prediction of the next 
        probable position based on tracking features. On the other 
        hand, in deep learning based methods like RCNN, fast RCNN, 
        which is a two-stage method and one staged method like YOLO.
        This research conducts a comparative analysis of traditional 
        computer vision 
        <p class="justified">Both techniques exhibit limitations and 
            advantages 
            associated with hardware and software. While classical computer vision
             methods struggle with occlusion and lighting problems, they serve well in terms of computation power. Machine learning based methods give robust solutions and are not affected by the surrounding environment. But they are highly dataset dependent and need a lot of computational capability to train huge models. The foundation of the proposed research lies in evaluating the above two models, via both qualitative and quantitative analysis, to perform real-time vehicle detection and tracking.
            and deep learning based vehicle detection and tracking algorithms using UAV for real-time traffic monitoring application. In conventional computer vision methods, the Haar cascade method is used with the Kannade Lucas optical flow tracker. This technique is compared with deep learning-based methods YoloV7 with DeepSORT to detect and track vehicles. A qualitative and quantitative analysis is performed in simulation and then tested on an in-house built UAV. The goal is to support informed decision making in perception and planning, ultimately enhancing safety in the industry. By evaluating these methods on a UAV platform, the research provides valuable insights for industry stakeholders to choose the most suitable approach for their specific requirements.
        
        Both techniques exhibit limitations and advantages associated with hardware and software. While classical computer vision methods struggle with occlusion and lighting problems, they serve well in terms of computation power. Machine learning based methods give robust solutions and are not affected by the surrounding environment. But they are highly dataset dependent and need a lot of computational capability to train huge models. The foundation of the proposed research lies in evaluating the above two models, via both qualitative and quantitative analysis, to perform real-time vehicle detection and tracking.</p>
    <br><br>


    <!-- Approaches -->
    <h3 id="Approaches" class ="Approaches">Approaches</h3>
    <h3 class="justified">Deep learning approach</h3>

    <h3 class="justified">Overview of YOLOv7:</h3>
<p class="justified">YOLOv7 is an advanced version of the YOLO architecture. Unlike traditional two-stage detectors like R-CNN, which first select region proposals and then classify them, YOLOv7 is a single-stage detector that predicts both bounding boxes and class probabilities directly from the image in one evaluation. This makes it exceptionally fast and suitable for real-time applications.
</p>
      <h3 class="justified">Overview of DeepSort:</h3>
<p class="justified"> DeepSort is an extension of the SORT (Simple Online and Realtime Tracking) algorithm. While SORT uses Kalman filtering and Hungarian algorithm for tracking, DeepSort incorporates deep learning features to improve tracking performance, especially in cases of occlusions or interactions between objects.
</p>

     <h3 class="justified"> Tracking with Deep SORT:</h3>

<h4 class="justified"> Initializing Trackers: </h4> <p class="justified"> When an object is detected for the first time, Deep SORT initializes a new tracker for it. This tracker uses the extracted features to keep track of the object. </p>
<h4 class="justified"> Data Association:</h4> <p class="justified"> In each new frame, Deep SORT performs a data association step. It compares the new detections (from YOLO) with existing trackers. This comparison is based on both the appearance features and the predicted motion of the objects. The Kalman filter is used for predicting the motion.</p>
<h4 class="justified">Matching:</h4> <p class="justified"> Deep SORT matches the new detections with existing trackers. If a detection matches an existing tracker, it updates the tracker's state (like its new position and appearance). If there are detections that don't match any existing tracker, it creates new trackers for them.</p>
<h4 class="justified"> Handling Lost Tracks:</h4> <p class="justified"> Sometimes, an object might be occluded or move out of the frame. Deep SORT handles this by allowing trackers to exist for a short time without new matching detections, giving the object a chance to reappear. If a tracker doesn't get a matching detection for too long, it is removed.</p>
    <img  src="deepsort_blockdiag.png" alt="Block Diagram of Deep SORT (Reference of paper https://www.hindawi.com/journals/cin/2023/7974201/fig1/ )" style="width: 50%; height: auto;">
    <img src="sample_input.png" alt="Block Diagram of Deep SORT (Reference of paper https://www.hindawi.com/journals/cin/2023/7974201/fig1/ )" style="width: 50%; height: auto;">
    <img src="sample_output.png" alt="Block Diagram of Deep SORT (Reference of paper https://www.hindawi.com/journals/cin/2023/7974201/fig1/ )" style="width: 50%; height: auto;">
    

      
      <br><br>


        <!-- Implementation -->
        <h3 id="Implementation" class ="Implementation">Implementation</h3>
        <p class="justified">Classical computer vision methods, which leverage inherent image attributes such as texture, color, and shape, have been the mainstay for years. Vehicles, with their distinct symmetrical shapes and unique colors, are particularly amenable to these techniques. There are several traditional computer vision methods that are being used in vehicle detection. Researchers have used methods such as template matching and Haar cascade, HOG based features, SIFT, ORB with classifiers like KNN, SVM to do vehicle detection. While template matching provides good results, Haar cascade is a better approach as it detects the objects by detecting features. HOG and Haar cascades give competitive results. Detected objects can be tracked with methods like Lucas-Kanade optical flow, Kalman filter based SORT, mean-shift tracking. These methods are mostly based on prediction of the next probable position based on tracking features. On the other hand, in deep learning based methods like RCNN, fast RCNN, which is a two-stage method and one staged method like YOLO.</p>
    
        <br><br>


            <!-- Results -->
    <h3 id="Results" class ="Results">Results and Conclusion</h3>
    <p class="justified">Classical computer vision methods, which leverage inherent image attributes such as texture, color, and shape, have been the mainstay for years. Vehicles, with their distinct symmetrical shapes and unique colors, are particularly amenable to these techniques. There are several traditional computer vision methods that are being used in vehicle detection. Researchers have used methods such as template matching and Haar cascade, HOG based features, SIFT, ORB with classifiers like KNN, SVM to do vehicle detection. While template matching provides good results, Haar cascade is a better approach as it detects the objects by detecting features. HOG and Haar cascades give competitive results. Detected objects can be tracked with methods like Lucas-Kanade optical flow, Kalman filter based SORT, mean-shift tracking. These methods are mostly based on prediction of the next probable position based on tracking features. On the other hand, in deep learning based methods like RCNN, fast RCNN, which is a two-stage method and one staged method like YOLO.</p>

    <br><br>


                <!-- Reference -->
    <h3 id="Reference" class ="Reference">Reference</h3>

    <br><br>
            
    
<div class="footer">
  <p>© Diksha Aggarwal and Surafel Anshebo</p>
</div>

</body>
</html>
